# -*- coding: utf-8 -*-
"""MetinOzet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y0NDsLNen76NWFOXthw7W5pYjEI_Cs9K

# Kütüphanelerin İmport Edilmesi
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from wordcloud import WordCloud
from gensim.models import Word2Vec
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import heapq
from gensim.summarization.summarizer import summarize
from gensim.summarization import keywords
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
# %matplotlib inline
from gensim.models import KeyedVectors
import networkx as nx

"""# Veri Setinin İçeri Aktarılması"""

#Veri setimize buradan ulaşabilirsiniz: https://www.kaggle.com/datasets/dilemre/turkish-news-article

all_news = pd.read_csv('TurkishNewsArticles.csv')

all_news

#Veri setimiz 2415 satırdan oluşmaktadır.

"""### Metnin cümlelere ve kelimelere ayrılması işlemi"""

import nltk
nltk.download('punkt')
all_news['Tokenized_Text'] = all_news['text'].apply(word_tokenize)
all_news['Tokenized_Sentence'] = all_news['text'].apply(sent_tokenize)
all_news

"""## Metin Temizleme"""

# Nltk kütüphanesinin türkçe için oluşturduğu stopwordsleri kullandık.

nltk.download('stopwords')
stop_words = stopwords.words('Turkish')
stop_words.append("bir")
corpus = []
for news in all_news['Tokenized_Text']:
    temp_list = []
    for word in news:
        if ((word.lower() not in stop_words) and (len(word) > 2)):
            temp_list.append(word)
    corpus.append(temp_list[:-22])

"""### Metni Görselleştirme"""

# Stopwordlerden temizlenmiş metnin görsel hali
for i in range(5):
    cloud_for_text = WordCloud().generate(' '.join(corpus[i]))
    plt.imshow(cloud_for_text, interpolation="bilinear")
    plt.axis("off")
    plt.show()

"""## Kelime Frekansı Algoritması"""

# Kelime frekansı algoritması
def summarize_func(words,sentences):
    word2count = {}
    for word in words:
        if word.lower() not in word2count.keys():
            word2count[word.lower()] = 1
        else:
            word2count[word.lower()] += 1

    maxi = max(word2count.values())

    for key in word2count.keys():
        word2count[key] = word2count[key]/maxi

    sent2score = {}
    for sentence in sentences:
        for word in word_tokenize(sentence.lower()):
            if word in word2count.keys():
                if len(sentence.split(' ')) < 15:
                    if sentence not in sent2score.keys():
                        sent2score[sentence] = word2count[word]
                    else:
                        sent2score[sentence] += word2count[word]

    best_sentences = heapq.nlargest(5, sent2score, key=sent2score.get)
    return[' '.join(best_sentences)]

all_news['Summarized_Text'] = ""
for i in range(all_news.shape[0]):
    all_news['Summarized_Text'][i] = summarize_func(all_news['Tokenized_Text'][i],all_news['Tokenized_Sentence'][i])

haber_numarasi = 2000
text = all_news.iloc[haber_numarasi].text
text

# Eşsiz kelimeler
def unik_word(sentences):
  unik=[]
  for sents in sentences:
    for word in word_tokenize(sents):
      if word not in unik:
        unik.append(word)
  return unik
unik_words=unik_word(text)
print(len(unik_words))

# Kelime frekanslerının görsel olarak temsili.
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
def frequency_of_words(sentences):
  freq_info=[]
  for i,sent in enumerate(sentences):
    words=word_tokenize(sent)
    frequency_dict={}
    for word in words:
      if word in frequency_dict.keys():
        frequency_dict[word]=frequency_dict[word]+1
      else:
        frequency_dict[word]=1
    temp={"sent_id":i,"frequency_of_word":frequency_dict}
    freq_info.append(temp)

  return freq_info

freq_info=frequency_of_words(text)

height=[]
left=[]
unik_words=unik_word(text)
for i,word in enumerate(unik_words):
  count=0
  for sent in text:
    words=word_tokenize(sent)
    for word_1 in words:
      if word==word_1:
        count=count+1
  left.append(i)
  height.append(count)

plt.bar(left, height, tick_label = unik_words,
        width = 0.4, color = ['red'])
plt.xlabel('x - axis')
# naming the y-axis
plt.ylabel('y - axis')
plt.show()

#print(freq_info)

haber_numarasi = 2000
text = all_news.iloc[haber_numarasi].text
text

cleaned_text = [word for word in word_tokenize(text) if (word not in stop_words and len(word) > 2)]

summary = summarize_func(cleaned_text,sent_tokenize(text))
summary

"""### Keywordsler"""

print ('Summary:')
print (summary)

print ('\nKeywords:')
print (keywords(' '.join(cleaned_text), ratio=0.05))

"""## TextRank Algoritması"""

# Algoritma için word2vec modelinin türkçe olarak eğitilmiş versiyonunu kullandık.
# Modelin import edilmesi
from gensim.models import KeyedVectors
word_tr = KeyedVectors.load_word2vec_format('trmodel', binary=True)

sentences = [sentence for sentence in text.split("\n") if len(sentence) > 30]
clean_sentences = []
for sentence in sentences:
    temp_list = []
    for word in sentence.split():
        if (word.lower() not in stop_words) and (len(word) >= 2):
            temp_list.append(word)
    clean_sentences.append(' '.join(temp_list))

# Vektörlerştirme.
sentence_vectors = []
for sentence in clean_sentences:
    for word in sentence.split():
        try:
            v = word_tr[word.lower()]
            #print(1, end="")
        except:
            v = np.zeros(400)
    sentence_vectors.append(v)

# Benzerlik matrisinin oluşturulması.
sim_mat = np.zeros([len(sentences), len(sentences)])

for i in range(len(sentences)):
    for j in range(len(sentences)):
        if i != j:
            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,400), sentence_vectors[j].reshape(1,400))[0,0]

#Benzerlik matrisine göre oluşturulan graf skorları
nx_graph = nx.from_numpy_array(sim_mat)
scores = nx.pagerank(nx_graph)
scores

#cumlelerin puanlanmasi
ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)

haber_numarasi = 2000
text = all_news.iloc[haber_numarasi].text
text

summary = []
for i in range(1):
    summary.append(ranked_sentences[i][1])
    print(ranked_sentences[i][1])

summary = " ".join(summary)
summary

"""# Performans Metrikleri"""

pip install rouge

from rouge import Rouge
ROUGE = Rouge()
ROUGE.get_scores(summary, text)